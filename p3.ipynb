{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c38507",
   "metadata": {
    "id": "xMC9hGGknkad"
   },
   "source": [
    "# P3 \n",
    "\n",
    "\n",
    "This project gives you experience with getting data from the web. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c3e907",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1638973997163,
     "user": {
      "displayName": "Laura Brown",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYbC5So3kNOYBteW-fSY5JpgP-bBJa7kyAEluaKg=s64",
      "userId": "02180634349638765126"
     },
     "user_tz": 300
    },
    "id": "4i05x2H2uD9z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "\n",
    "if re.search(r'amzn', os.uname().release):\n",
    "    GS = True\n",
    "else:\n",
    "    GS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e01932",
   "metadata": {},
   "source": [
    "**NOTE** A note on grading for this project.  \n",
    "Each question has the total points listed which will be split between autograding methods and manual grading; make sure you review your code to achieve the requested result.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11245455",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3Gnokidqn_H7"
   },
   "source": [
    "## Q1 - (6 pts total)\n",
    "\n",
    "Scrape [https://www.data.gov](https://www.data.gov) to identify and print out the number of data sets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f32087",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1638977359150,
     "user": {
      "displayName": "Laura Brown",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYbC5So3kNOYBteW-fSY5JpgP-bBJa7kyAEluaKg=s64",
      "userId": "02180634349638765126"
     },
     "user_tz": 300
    },
    "id": "MLg0cuqsoMU8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "site = requests.get('https://www.data.gov').text\n",
    "q1 = BeautifulSoup(site, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a7062d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datasets is  301368\n"
     ]
    }
   ],
   "source": [
    "\n",
    "q1_num_datasets = int(q1('span','text-color-red')[0].text.strip().replace(',',''))\n",
    "\n",
    "print(\"The number of datasets is \", str(q1_num_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891fa472",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! âœ¨</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e86709",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3mJlHZc8pYNK"
   },
   "source": [
    "## Q2 - (30 pts total) \n",
    "\n",
    "From the list of top 200 most popular names for babies born in the US in the 2010s.  We are interested in the names that start with 'W' or contain a letter  'P' or 'p'.  \n",
    "\n",
    "Create a DataFrame with the names that start with 'W' or contain a letter  'P' or 'p' (for both male and female together), their rank, and the number of times the name is used.  DataFrame has four columns: \"Rank\" - int, \"Names\" - str, \"Number\" - int, and \"Gender\" - str - \"Male\"/\"Female\". \n",
    "\n",
    "Examples names from the table are: \n",
    "* 3, Sophia, 181091, Female\n",
    "* 4, William, 159893, Male\n",
    "* ...\n",
    "* 37, Wyatt, 87711, Male\n",
    "* ...\n",
    "* 44, Penelope, 47602, Female\n",
    "* ... \n",
    "\n",
    "Display the first 10 rows of the DataFrame, sorted by decreasing number.  Also, print out the sum of the number of the 'W' names and the sum of the number of names with 'P' or 'p'.  \n",
    "\n",
    "Data available at: https://www.ssa.gov/OACT/babynames/decades/names2010s.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7f6a39",
   "metadata": {
    "id": "59rffn1hqxj1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "site = requests.get('https://www.ssa.gov/OACT/babynames/decades/names2010s.html').text\n",
    "q2 = BeautifulSoup(site, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc074a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1633719248228,
     "user": {
      "displayName": "Laura Brown",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYbC5So3kNOYBteW-fSY5JpgP-bBJa7kyAEluaKg=s64",
      "userId": "02180634349638765126"
     },
     "user_tz": 240
    },
    "id": "FQ-XVx5_qxXO",
    "outputId": "acdaee3f-515f-4e0c-83a1-3c1bb423d930",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of names that start with W  : 334604\n",
      "Total number of names that contain P/p   : 915013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Num</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Sophia</td>\n",
       "      <td>181091</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>William</td>\n",
       "      <td>159893</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>115642</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>Christopher</td>\n",
       "      <td>102084</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Wyatt</td>\n",
       "      <td>87711</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Harper</td>\n",
       "      <td>85168</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>83</td>\n",
       "      <td>Cooper</td>\n",
       "      <td>49266</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>84</td>\n",
       "      <td>Parker</td>\n",
       "      <td>49220</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44</td>\n",
       "      <td>Penelope</td>\n",
       "      <td>47602</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>Peyton</td>\n",
       "      <td>39603</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank         Name     Num  Gender\n",
       "0      3       Sophia  181091  Female\n",
       "1      4      William  159893    Male\n",
       "3     20       Joseph  115642    Male\n",
       "4     27  Christopher  102084    Male\n",
       "5     37        Wyatt   87711    Male\n",
       "2     16       Harper   85168  Female\n",
       "10    83       Cooper   49266    Male\n",
       "11    84       Parker   49220    Male\n",
       "6     44     Penelope   47602  Female\n",
       "7     64       Peyton   39603  Female"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame 'names' for the names that start with W or have a P/p\n",
    "# The 'names' DataFrame has four colummns: \n",
    "#   'Rank' (int) \n",
    "#   'Name' (str)\n",
    "#   'Num' (int) \n",
    "#   'Gender' (str) 'Female' / 'Male'\n",
    "table = q2.find_all('table', 't-stripe')[0]\n",
    "\n",
    "data = [\n",
    "    [int(row.find_all('td')[0].text), name, int(row.find_all('td') [col].text.replace(',', '')), gender]\n",
    "    for row in table.find_all('tr') [1:]\n",
    "    if len(row.find_all('td')) >= 5\n",
    "    for name, col, gender in [ \n",
    "        (row.find_all('td') [1].text, 2, 'Male'),\n",
    "        (row.find_all('td') [3].text, 4, 'Female')\n",
    "    ]\n",
    "    if name.startswith('W') or 'p' in name.lower()\n",
    "]\n",
    "\n",
    "names = pd.DataFrame(data, columns=['Rank', 'Name', 'Num', 'Gender']).sort_values (by='Num', ascending=False) \n",
    "\n",
    "numWnames = names[names['Name'].str.startswith('W')]['Num'].sum() \n",
    "numPnames = names[names['Name'].str.contains('P', case=False)]['Num'].sum()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total number of names that start with W  : \" + str(numWnames))\n",
    "print(\"Total number of names that contain P/p   : \" + str(numPnames))\n",
    "\n",
    "names.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08bf4af3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! ðŸŽ‰</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d366e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JzLFYsEgq1kx"
   },
   "source": [
    "## Q3 - (54 pts total) \n",
    "\n",
    "Browse through the following fake online bookstore: http://books.toscrape.com/. This website is meant for toying with scraping.\n",
    "\n",
    "Your job is to scrape the website, collecting data on all books that have:\n",
    "- **_at least_ a four-star rating**, and\n",
    "- **a price _strictly_ less than Â£50**, and \n",
    "- **belong to specific categories** (more details below). \n",
    "\n",
    "You will extract the information into a DataFrame that looks like the one below.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>UPC</th>\n",
    "      <th>Product Type</th>\n",
    "      <th>Price (excl. tax)</th>\n",
    "      <th>Price (incl. tax)</th>\n",
    "      <th>Tax</th>\n",
    "      <th>Availability</th>\n",
    "      <th>Number of reviews</th>\n",
    "      <th>Category</th>\n",
    "      <th>Rating</th>\n",
    "      <th>Description</th>\n",
    "      <th>Title</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>e10e1e165dc8be4a</td>\n",
    "      <td>Books</td>\n",
    "      <td>Â£22.60</td>\n",
    "      <td>Â£22.60</td>\n",
    "      <td>Â£0.00</td>\n",
    "      <td>In stock (19 available)</td>\n",
    "      <td>0</td>\n",
    "      <td>Default</td>\n",
    "      <td>Four</td>\n",
    "      <td>For readers of Laura Hillenbrand's Seabiscuit...</td>\n",
    "      <td>The Boys in the Boat: Nine Americans...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>c2e46a2ee3b4a322</td>\n",
    "      <td>Books</td>\n",
    "      <td>Â£25.27</td>\n",
    "      <td>Â£25.27</td>\n",
    "      <td>Â£0.00</td>\n",
    "      <td>In stock (19 available)</td>\n",
    "      <td>0</td>\n",
    "      <td>Romance</td>\n",
    "      <td>Five</td>\n",
    "      <td>A Michelin two-star chef at twenty-eight, Violette...</td>\n",
    "      <td>Chase Me (Paris Nights #2)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>00bfed9e18bb36f3</td>\n",
    "      <td>Books</td>\n",
    "      <td>Â£34.53</td>\n",
    "      <td>Â£34.53</td>\n",
    "      <td>Â£0.00</td>\n",
    "      <td>In stock (19 available)</td>\n",
    "      <td>0</td>\n",
    "      <td>Romance</td>\n",
    "      <td>Five</td>\n",
    "      <td>No matter how busy he keeps himself...</td>\n",
    "      <td>Black Dust</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "To do so, implement the following functions.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `extract_book_links`\n",
    "\n",
    "Complete the implementation of the function `extract_book_links`, which takes in the content of a page that contains book listings as a **string of HTML**, and returns a **list** of URLs of book-specific pages for all books with **_at least_ a four-star rating and a price _strictly_ less than Â£50**.\n",
    "\n",
    "For this method, the URLs you return should not contain the protocol (i.e. `'https://'`). The protocols should be added back into the URLs when you actually make the requests.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `get_product_info`\n",
    "\n",
    "Complete the implementation of the function `get_product_info`, which takes in the content of a book-specific page as a **string of HTML**, and a list `categories` of book categories. If the input book is in the list of `categories`, `get_product_info` should return a dictionary corresponding to a row in the DataFrame in the image above (where the keys are the column names and the values are the row values). If the input book is not in the list of `categories`, return `None`.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `scrape_books`\n",
    "\n",
    "Finally, put everything together. Complete the implementation of the function `scrape_books`, which takes in an integer `k` and a list `categories` of book categories. `scrape_books` should use `requests` to scrape the first `k` pages of the bookstore and return a DataFrame of only the books that have \n",
    "- **_at least_ a four-star rating**, and\n",
    "- **a price _strictly_ less than Â£50**, and\n",
    "- **a category that is in the list `categories`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "Some general guidance and tips:\n",
    "\n",
    "- The first page of the bookstore is at http://books.toscrape.com/catalogue/page-1.html. Subsequent pages can be found by clicking the \"Next\" button at the bottom of the page. Look at how the URLs change each time you navigate to a new page; think about how to use f-strings (or some other string formatting technique) to generate these URLs.\n",
    "- Use \"inspect element\" to view the source code of the pages you're trying to scrape. To find a book's category, look at the hyperlinks in the book-specific page for that book.\n",
    "- **`scrape_books` should run in under 180 seconds on the entire bookstore (`k = 50`). `scrape_books` is also the only function that should make `GET` requests; the other two functions parse already-existing HTML.**\n",
    "- When instantiating `bs4.BeautifulSoup` objects, use the optional argument `features='lxml'` to suppress any warnings.\n",
    "- Don't worry about typecasting, i.e. it's fine if `'Number of reviews'` is not stored as type `int`. Also, don't worry if you run into encoding errors in your price columns (as the example DataFrame at the top of this cell contains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946c9950-a22a-4c9f-97bf-f9943e07593d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_book_links(text):\n",
    "    soup = BeautifulSoup(text, features='lxml')\n",
    "    book_links = []\n",
    "\n",
    "    for article in soup.select('article.product_pod'):\n",
    "        rating = article.find('p', class_='star-rating')['class'][1]\n",
    "        price = float(article.find('p', class_='price_color').text[1:].replace('Â£', ''))\n",
    "\n",
    "        rating_value = ['One', 'Two', 'Three', 'Four', 'Five'].index(rating) + 1\n",
    "        if rating_value >= 4 and price < 50:\n",
    "            link = article.find('h3').a['href']\n",
    "            book_links.append(link.replace('catalogue/', ''))\n",
    "\n",
    "    return book_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f55bd3c-73bb-4484-b4bb-4ed2facf02b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_product_info(text, categories):\n",
    "    soup = BeautifulSoup(text, features='lxml')\n",
    "    category = soup.select_one('ul.breadcrumb li:nth-of-type(3) a').text\n",
    "    if category not in categories:\n",
    "        return None\n",
    "\n",
    "    title = soup.h1.text\n",
    "    rating = soup.find('p', class_='star-rating')['class'][1]\n",
    "    table_data = soup.select('table.table.table-striped td')\n",
    "    upc = table_data[0].text\n",
    "    price_excl_tax = table_data[2].text\n",
    "    price_incl_tax = table_data[3].text\n",
    "    tax = table_data[4].text\n",
    "    availability = table_data[5].text\n",
    "    num_reviews = table_data[6].text\n",
    "\n",
    "    description_tag = soup.select_one('#product_description ~ p')\n",
    "    description = description_tag.text if description_tag else 'No description available'\n",
    "\n",
    "    product_info = {\n",
    "        'UPC': upc,\n",
    "        'Product Type': 'Books',\n",
    "        'Price (excl. tax)': price_excl_tax,\n",
    "        'Price (incl. tax)': price_incl_tax,\n",
    "        'Tax': tax,\n",
    "        'Availability': availability,\n",
    "        'Number of reviews': num_reviews,\n",
    "        'Category': category,\n",
    "        'Rating': rating,\n",
    "        'Description': description,\n",
    "        'Title': title\n",
    "    }\n",
    "\n",
    "    return product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1b4248-f204-4c75-9da3-8f2f17b9246f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_books(k, categories):\n",
    "    book_data = []\n",
    "    base_url = \"http://books.toscrape.com/catalogue/\"\n",
    "\n",
    "    for page in range(1, k + 1):\n",
    "        page_url = f\"{base_url}page-{page}.html\"\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        for link in extract_book_links(response.text):\n",
    "            book_url = f\"{base_url}{link}\"\n",
    "            book_response = requests.get(book_url)\n",
    "            book_response.raise_for_status()\n",
    "            book_info = get_product_info(book_response.text, categories)\n",
    "            if book_info:\n",
    "                book_data.append(book_info)\n",
    "\n",
    "    return pd.DataFrame(book_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2598d6f9-316c-491f-944c-c8669939f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (5.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d245b18f-6776-4c96-b9ed-a9453930a3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# don't delete this cell, but do run it -- it is needed for the autograder tests\n",
    "\n",
    "# public test for extract_book_links \n",
    "extract_book_info_out = extract_book_links(\n",
    "    open('data/products.html', encoding='utf-8').read()\n",
    ")\n",
    "extract_book_url = 'scarlet-the-lunar-chronicles-2_218/index.html'\n",
    "\n",
    "# public tests for get product info\n",
    "product_info_out = get_product_info(open('data/Frankenstein.html', encoding='utf-8').read(), \n",
    "                                    ['Default'])\n",
    "\n",
    "# public test for scrape books \n",
    "scrape_books_out = scrape_books(1, ['Mystery'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1d699f-6781-4766-a45d-1b6c733687ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# don't delete this cell, but do run it -- it is needed for the autograder tests\n",
    "\n",
    "# more use of scrape books\n",
    "scrape_books_out2 = scrape_books(3, ['Mystery', 'History'])\n",
    "\n",
    "scrape_books_out3 = scrape_books(5, ['Default', 'Young Adult', 'Fiction'])\n",
    "\n",
    "scrape_books_out4 = scrape_books(15, ['Nonfiction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd01c60b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! ðŸŒŸ</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ab86f-e6f5-4a37-91b8-361c666c26ba",
   "metadata": {},
   "source": [
    "## Q4 \n",
    "\n",
    "For this problem, you will be getting data from an API.  We will be getting information from NASA about Near Earth Objects (NEOs).  \n",
    "\n",
    "First, you will need to get an API key to use the system.  You can get one at https://api.nasa.gov/. Youâ€™ll need to supply an email address, which can\n",
    "be either your MTU email or a personal email address.\n",
    "\n",
    "Be aware of the limitation on the requests per day from a given IP. \n",
    "\n",
    "Do not repeatedly hit the system for requests or you will shut down your access for the rest of the day. \n",
    "\n",
    "**Asteroids NeoWs API** \n",
    "\n",
    "We will be using the Asteroids NeoWs API.  You can find information on this API under the \"Browse APIs\" part of the website. \n",
    "\n",
    "We will use the \"feed\" endpoint to retrieve Near Earth Objects based on the\n",
    "date of their closest approach to Earth. This endpoint takes three parameters: \n",
    "`start_date`, `end_date` and `api_key`. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72602e6-5cb5-49e9-ae64-11b1866140c1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Q4a - (8 pts total)\n",
    "\n",
    "You will construct the request string to retrieve the Near Earth Objects for January 1, 2021.  \n",
    "\n",
    "From the response, report the number of NEOs - `num_NEOs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e90db-fe71-4984-8457-abd820e6f9d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if GS==False:\n",
    "    keys = {}\n",
    "    keys['NASA_KEY'] = getpass(\"Enter NASA API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85329e0d-b7cc-4c6c-8fbc-7b22cdf169af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4site = 'https://api.nasa.gov/neo/rest/v1/feed'\n",
    "q4a_start_date = '2021-01-01'\n",
    "q4a_end_date = '2021-01-01'\n",
    "q4a_api_key = keys['NASA_KEY']\n",
    "\n",
    "q4a_params = {\n",
    "    'start_date':q4a_start_date,\n",
    "    'end_date': q4a_end_date,\n",
    "    'api_key':q4a_api_key  \n",
    "}\n",
    "\n",
    "q4a_resp = requests.get(q4site, q4a_params)\n",
    "data = q4a_resp.json()\n",
    "num_NEOs = sum(len(data['near_earth_objects'][date]) for date in data['near_earth_objects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242499d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02035046-b09f-4d35-acff-b46efaeb4c1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Q4b - (12 pts total)\n",
    "\n",
    "You are writing a function to get all the Near Earth Objects for a particular day from the API.  \n",
    "\n",
    "The function `get_neos_response` has 4 inputs: \n",
    "\n",
    "* `key` - string, API Key \n",
    "* `year` - integer \n",
    "* `month` - integer, optional defaults to 1\n",
    "* `day` - integer, optional defaults to 1\n",
    "\n",
    "The function returns the JSON object from the NeoWs endpoint for the specified day. \n",
    "\n",
    "*Assumptions*   \n",
    "You may assume the `year`, `month`, `day` arguments are positive integers and that when formatted as `yyyy-mm-dd` specify a valid date. \n",
    "\n",
    "You make make use of any of the libraries imported at the top of the notebook, e.g., `datetime`, but may not import other libraries. \n",
    "\n",
    "*Suggestions*  \n",
    "Break into steps: \n",
    "\n",
    "1. Use the supplied date informaiton to construct the `start_date` and `end_date` strings needed for the API request. \n",
    "\n",
    "2. Use the `start_date` and `end_date` along with the API key to create a dictionary of parameters, `q4b_params`, and use `requests` to submit a HTTP GET request to the NASA API. \n",
    "\n",
    "3. Extract the JSON object in the resulting `requests` object and return it as a dictionary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d546105-b566-49d2-bcd9-286432cf02bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function get_neos_response\n",
    "# Inputs: \n",
    "#  key - string, API key\n",
    "#  year - integer, required \n",
    "#  month - integer, optional default to 1\n",
    "#  day - integer, optional default to 1\n",
    "# Returns: \n",
    "#  JSON object that is returned by NASA NeoWs end point for specified day \n",
    "\n",
    "def get_neos_response(key, year, month=1, day=1): \n",
    "\n",
    "    q4b_start_date =q4b_end_date=f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    \n",
    "    q4b_params = {\n",
    "        'start_date': q4b_start_date,\n",
    "        'end_date': q4b_end_date,\n",
    "        'api_key': key\n",
    "        }\n",
    "    \n",
    "    q4b_resp = requests.get('https://api.nasa.gov/neo/rest/v1/feed', params=q4b_params)\n",
    "    return q4b_resp.json()\n",
    "\n",
    "\n",
    "q4b_test1 = get_neos_response(keys['NASA_KEY'], 2022)\n",
    "q4b_test2 = get_neos_response(keys['NASA_KEY'], 1986, 2, 9)\n",
    "q4b_test3 = get_neos_response(keys['NASA_KEY'], 1958, 12)\n",
    "q4b_test4 = get_neos_response(keys['NASA_KEY'], 2021)\n",
    "q4b_test5 = get_neos_response(keys['NASA_KEY'], 2015, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbd25c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd185733-0c7b-4845-94f5-0f6e9fcff8d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Q4c - (14 pts total)\n",
    "\n",
    "Now you will be asked to write a function `is_valid_date` that takes three input: \n",
    "\n",
    "* `year`, integer\n",
    "* `month`, integer \n",
    "* `day`, integer \n",
    "\n",
    "and returns a Boolean that is true if and only if the arguments are for a valid date. \n",
    "\n",
    "\n",
    "Examples of valid dates are: \n",
    " \n",
    " * 2021-10-31\n",
    " * 1934-02-20\n",
    " * 1968-07-30\n",
    " \n",
    "Examples of invalid dates are: \n",
    "\n",
    " * 2020-13-01 *no 13th month*\n",
    " * 2004-04-34 *no 34th day* \n",
    " * 1988-00-04 *no 0th month* \n",
    " * 1997-02-29 *no leap day* \n",
    " \n",
    "Accounting for [leap years](https://en.wikipedia.org/wiki/Leap_year) is more challenging.  Leap years (in the Gregorian calendar) are those that are divisible by 4, but not by 100, except if they are divisible by 400.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727d4a2-02dd-4777-a5ed-6e5824d870f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function is_valid_date\n",
    "# Inputs: \n",
    "#  year - integer\n",
    "#  month - integer\n",
    "#  day - integer\n",
    "# Returns: \n",
    "#  Boolean - true if and only if the arguments are for a valid date  \n",
    "from datetime import datetime\n",
    "\n",
    "def is_valid_date(year,month,day): \n",
    "       try:\n",
    "        datetime(year, month, day)\n",
    "        return True\n",
    "       except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "q4c_test1 = is_valid_date(2023, 11, 9)\n",
    "q4c_test2 = is_valid_date(2020, 10, 32)\n",
    "q4c_test3 = is_valid_date(2020, 0, 1)\n",
    "q4c_test4 = is_valid_date(2023, 2, 29)\n",
    "q4c_test5 = is_valid_date(2016, 2, 29)\n",
    "q4c_test6 = is_valid_date(1990, 2, 29)\n",
    "q4c_test7 = is_valid_date(2020, 2, 29)\n",
    "q4c_test8 = is_valid_date(2000, 2, 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f79b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65410714-70bf-49c1-838e-c6f53226fd25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Q4d - (24 pts total)\n",
    "\n",
    "Now you will write a function `get_neos_data` that takes the same arguments as `get_neos_response`, but now returns a DataFrame with some of the Near Earth Objects information. \n",
    "\n",
    "The function `get_neos_data` has 4 inputs:\n",
    "\n",
    "* `key` - string, API Key\n",
    "* `year` - integer\n",
    "* `month` - integer, optional defaults to 1\n",
    "* `day` - integer, optional defaults to 1\n",
    "\n",
    "You should check that the arguments are of the appropriate type and are all positive, if not return None. Additionally, if the arguments do not specify a valid date, return None. \n",
    "\n",
    "Use the `get_neos_response` function to get the JSON object for the date from NASA. \n",
    "\n",
    "Extract the following information into a DataFrame for the Near Earth Objects and return that DataFrame: \n",
    "\n",
    "* `id` the \"id\" of the NEO \n",
    "* `name` the \"name\" of the NEO \n",
    "* `minDim` the \"minimal diameter\" of the NEO in kilometers  (float)\n",
    "* `maxDim` the \"maximal diameter\" of the NEO in kilometers  (float)\n",
    "* `missDist` the \"miss distance\" of the NEO in kilometers  (float)\n",
    "* `orbitBody` the \"orbiting body\" of the NEO\n",
    "* `hazard` if the NEO \"is potentially hazardous\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd939300-3f53-4107-85a3-cf3cca53bba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function get_neos_data\n",
    "# Inputs: \n",
    "#  key - string, API key\n",
    "#  year - integer, required \n",
    "#  month - integer, optional default to 1\n",
    "#  day - integer, optional default to 1\n",
    "# Returns: \n",
    "#  JSON object that is returned by NASA NeoWs end point for specified day \n",
    "def get_neos_data(key,year,month=1,day=1): \n",
    "\n",
    "    # Check inputs \n",
    "    x=is_valid_date(year,month,day)\n",
    "    \n",
    "    # Get JSON Object with get_neos_response\n",
    "    response = get_neos_response(key, year, month, day)\n",
    "    \n",
    "    # Collect informaiton into DataFrame \n",
    "    neos_list = []\n",
    "    for date, neos in response.get('near_earth_objects', {}).items():\n",
    "        for neo in neos:\n",
    "            neos_list.append({\n",
    "                'id': neo['id'],'name': neo['name'],'minDim': neo['estimated_diameter']['kilometers']['estimated_diameter_min'],\n",
    "                'maxDim': neo['estimated_diameter']['kilometers']['estimated_diameter_max'],\n",
    "                'missDist': float(neo['close_approach_data'][0]['miss_distance']['kilometers']),\n",
    "                'orbitBody': neo['close_approach_data'][0]['orbiting_body'],'hazard': neo['is_potentially_hazardous_asteroid']\n",
    "            })\n",
    "    df=pd.DataFrame(neos_list)\n",
    "    if not neos_list:\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "q4d_df1 = get_neos_data(keys['NASA_KEY'], 2022)\n",
    "q4d_df2 = get_neos_data(keys['NASA_KEY'], 1986, 2, 9)\n",
    "q4d_df3 = get_neos_data(keys['NASA_KEY'], 1958, 12)\n",
    "q4d_df4 = get_neos_data(keys['NASA_KEY'], 2021)\n",
    "q4d_df5 = get_neos_data(keys['NASA_KEY'], 2015, 1, 40)\n",
    "q4d_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7212f-3b74-4eb4-a300-48a2b302e168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del keys  # remove api key from being saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729fdf2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969f188",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Bonus - (4 points) \n",
    "\n",
    "Going back to the names data from Q3.  Let's look at how the distribution of popular names has changed over time. \n",
    "\n",
    "Create a small multiples plot (meaning each plot is the same type and showing similar information in a grid).  \n",
    "\n",
    "The small multiples plot will show how the distribution of names has changed from the 1960s to 2010s.  \n",
    "\n",
    "1960s data - https://www.ssa.gov/OACT/babynames/decades/names1960s.html  \n",
    "1970s data - https://www.ssa.gov/OACT/babynames/decades/names1970s.html  \n",
    "...  \n",
    "2010s data - https://www.ssa.gov/OACT/babynames/decades/names2010s.html\n",
    "\n",
    "Let's focus on just how the distribution of girls names have changed.  This can be displayed using a line plot on just the top 100 names per decade.\n",
    "\n",
    "When getting the information, you should also scrape the total number of female births in that period to normalize the number of names as a percent in that decade. \n",
    "\n",
    "\n",
    "Your final grid should display: \n",
    "\n",
    "|         | Col 1 | Col 2 | \n",
    "|-------|-----------|-----------|\n",
    "| Row 1 | 1960 plot | 1970 plot | \n",
    "| Row 2 | 1980 plot | 1990 plot | \n",
    "| Row 3 | 2000 plot | 2010 plot | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a9a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create small multiples plot. \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define a function to scrape names and counts from each decade page\n",
    "def scrape_decade_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Locate the table of names\n",
    "    table = soup.find('table', {'class': 't-stripe'})\n",
    "    \n",
    "    # Collect the names and counts into a list of dictionaries\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:101]:  # Only top 100 rows\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # Check if row has the expected number of columns\n",
    "        if len(cols) < 3:\n",
    "            continue  # Skip rows with insufficient data\n",
    "        \n",
    "        try:\n",
    "            rank = int(cols[0].text.strip())\n",
    "            name = cols[1].text.strip()\n",
    "            count = int(cols[2].text.strip().replace(\",\", \"\"))\n",
    "            data.append({'Rank': rank, 'Name': name, 'Count': count})\n",
    "        except ValueError:\n",
    "            continue  # Skip rows where parsing fails\n",
    "    \n",
    "    # Attempt to get the total female births by looking for a pattern\n",
    "    total_female_births = None\n",
    "    for p in soup.find_all('p'):\n",
    "        paragraph_text = p.get_text()\n",
    "        # Use regex to find total female births near phrases like \"female births\"\n",
    "        match = re.search(r\"Total female births:?\\s*([\\d,]+)\", paragraph_text)\n",
    "        if match:\n",
    "            total_female_births = int(match.group(1).replace(\",\", \"\"))\n",
    "            break\n",
    "    \n",
    "    # Debug print if total births are not found\n",
    "    if total_female_births is None:\n",
    "        print(\"Could not find total female births in paragraph text.\")\n",
    "        for p in soup.find_all('p'):\n",
    "            print(p.get_text())  # Print each paragraph to help debug\n",
    "\n",
    "    # Ensure we found the total female births; raise an error if not\n",
    "    if total_female_births is None:\n",
    "        raise ValueError(\"Total female births not found on page.\")\n",
    "    \n",
    "    return pd.DataFrame(data), total_female_births\n",
    "\n",
    "# URLs for each decade\n",
    "urls = {\n",
    "    '1960s': \"https://www.ssa.gov/OACT/babynames/decades/names1960s.html\",\n",
    "    '1970s': \"https://www.ssa.gov/OACT/babynames/decades/names1970s.html\",\n",
    "    '1980s': \"https://www.ssa.gov/OACT/babynames/decades/names1980s.html\",\n",
    "    '1990s': \"https://www.ssa.gov/OACT/babynames/decades/names1990s.html\",\n",
    "    '2000s': \"https://www.ssa.gov/OACT/babynames/decades/names2000s.html\",\n",
    "    '2010s': \"https://www.ssa.gov/OACT/babynames/decades/names2010s.html\"\n",
    "}\n",
    "\n",
    "# Loop through each decade URL and store the data in a dictionary\n",
    "decade_data = {}\n",
    "for decade, url in urls.items():\n",
    "    data, total_births = scrape_decade_data(url)\n",
    "    data['Percentage'] = data['Count'] / total_births * 100\n",
    "    decade_data[decade] = data\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all decades into one DataFrame with a new 'Decade' column\n",
    "all_data = pd.concat(\n",
    "    [df.assign(Decade=decade) for decade, df in decade_data.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the grid\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each decade\n",
    "for i, (decade, df) in enumerate(decade_data.items()):\n",
    "    ax = axes[i]\n",
    "    # Plot each name's rank as a line\n",
    "    ax.plot(df['Rank'], df['Percentage'], marker='o', linestyle='-', color='b', alpha=0.6)\n",
    "    ax.set_title(f\"{decade} (Top 100 Names)\")\n",
    "    ax.set_xlabel(\"Rank\")\n",
    "    ax.set_ylabel(\"Percentage of Female Births\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c9126-8860-4627-a2f7-0f7d33e4bc74",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished P3! \n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the P3 assignment on Gradescope. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Warning!</strong> \n",
    "    This lab notebook will be graded a bit differently.  The `requests` module does not run properly on Gradescope.  Normally, when you upload your submission to Gradescope, it runs your code and reports the results on the test cases.  \n",
    "</div>\n",
    "\n",
    "For this assignment, the results and variables you create in your notebook will be saved out to a log file `.OTTER_LOG`.  This file will be included in your zip, when you run the export function below. \n",
    "\n",
    "\n",
    "\n",
    "Make sure you have run all cells in your notebook **in order** before running the cell below, so that all information gets saved to the log file correctly. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "If you run the notebook repeatedly, more and more information gets added to the `.OTTER_LOG` file. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Warning! - Clean log file</strong>     \n",
    "    Before running your last single run through the notebook, clear all clear, restart the kernel, delete the `.OTTER_LOG` so that a fresh one is created. \n",
    "</div>\n",
    "\n",
    "Your `.OTTER_LOG` file and `p3.otter` should be part of the zip submission (the export function does this automatically).  The `.OTTER_LOG` file should be less than 5MB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4b461",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f54206",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53832e",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "p3",
   "tests": {
    "q1": {
     "name": "q1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(q1_num_datasets) == int\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 20,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(numWnames == 334604)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(numPnames == 915013)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> names.shape[0] == 22 and names.shape[1] == 4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(names.dtypes == ['int64', 'object', 'int64', 'object'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(names.columns == ['Rank', 'Name', 'Num', 'Gender'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": [
      1,
      1,
      2,
      2,
      2,
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> extract_book_info_out[1] == extract_book_url\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(product_info_out, dict)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'Category' in product_info_out.keys()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> product_info_out['Rating'] == 'Two'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> scrape_books_out.shape == (1, 11)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> scrape_books_out['Rating'][0] == 'Four'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> scrape_books_out['Title'][0] == 'Sharp Objects'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": [
      1,
      1,
      2,
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(q4a_resp) == requests.models.Response\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(q4a_resp.json()) == dict\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(q4a_resp.json().keys()) == ['links', 'element_count', 'near_earth_objects']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4a_params['start_date'] == '2021-01-01' and q4a_params['end_date'] == '2021-01-01'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> num_NEOs == 10\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(q4b_test1) == dict and type(q4b_test2) == dict and (type(q4b_test3) == dict)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(q4b_test1.keys()) == ['links', 'element_count', 'near_earth_objects'] and list(q4b_test2.keys()) == ['links', 'element_count', 'near_earth_objects'] and (list(q4b_test3.keys()) == ['links', 'element_count', 'near_earth_objects'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q4c_test1 == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4c_test2 == False\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4c_test3 == False\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 14,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(q4d_df1) == pd.core.frame.DataFrame and type(q4d_df2) == pd.core.frame.DataFrame\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4d_df5 == None\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4d_df1.shape == (23, 7)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(q4d_df4.dtypes == ['object', 'object', 'float64', 'float64', 'float64', 'object', 'bool'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
